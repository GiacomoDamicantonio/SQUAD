{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "SQUAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1V9SMkqyXdtXbzznffwsnUm2Y_mFMtEl3",
      "authorship_tag": "ABX9TyMRvJtP3F6P+8KG1fF0QY0F",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53c5a7fc4e434e5d894cd1c71af84978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_796be84da21b4941a35f20e9567a1aa8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e7f04cff294347249fcffa630135220d",
              "IPY_MODEL_12290231470c4132bd65a82a1d40e7ba"
            ]
          }
        },
        "796be84da21b4941a35f20e9567a1aa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e7f04cff294347249fcffa630135220d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_848eef3ec052492b84f05f97acfd7152",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 487203636,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 487203636,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87bdff13e0fa4aea9b7f16c678e3f47b"
          }
        },
        "12290231470c4132bd65a82a1d40e7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01ccbd1f3e384eba960190675640a702",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 487M/487M [00:08&lt;00:00, 55.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7ff1b85ea3848cbbf116c5d9735266d"
          }
        },
        "848eef3ec052492b84f05f97acfd7152": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87bdff13e0fa4aea9b7f16c678e3f47b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01ccbd1f3e384eba960190675640a702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7ff1b85ea3848cbbf116c5d9735266d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiacomoDamicantonio/SQUAD/blob/main/SQUAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmNYQXUuqadj",
        "outputId": "4032fb89-efc1-437e-d696-bc6eb5f19578"
      },
      "source": [
        "os.system(\"pip install -q \\\"transformers==4.3\\\"\")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 7.1MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 transformers-4.6.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcgqYqJAe-62"
      },
      "source": [
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text, all_answers, qid):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.qid = qid\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        # Clean context, answer and question\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "        # Find end character index of answer in context\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Mark the character indexes in context that are in answer\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        # Tokenize context\n",
        "        tokenized_context = tokenizer(context, return_offsets_mapping=True)\n",
        "\n",
        "        # Find tokens that were created from answer characters\n",
        "        offsets = tokenized_context.offset_mapping\n",
        "        ans_token_idx = []\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        # Find start and end token index for tokens from answer\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        # Tokenize question\n",
        "        tokenized_question = tokenizer(question, return_offsets_mapping=True)\n",
        "\n",
        "        # Create inputs\n",
        "        input_ids = tokenized_context.input_ids + tokenized_question.input_ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.input_ids) + [1] * len(\n",
        "            tokenized_question.input_ids[1:]\n",
        "        )\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Pad and create attention masks.\n",
        "        # Skip if truncation is needed\n",
        "        padding_length = max_len - len(input_ids)\n",
        "        if padding_length > 0:  # pad\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:  # skip\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = offsets\n",
        "\n",
        "def create_squad_examples(raw_data, errors):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "              if qa['id'] not in errors:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers, qa['id']\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "              else:\n",
        "                print(qa)\n",
        "    return squad_examples\n",
        "\n",
        "def create_eval_examples(raw_data, errors):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"][:5]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "              if qa['id'] not in errors:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers, qa['id']\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "\n",
        "    return squad_examples\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1lau4PTOWUe"
      },
      "source": [
        "max_len = 512\n",
        "\n",
        "# Save the slow pretrained tokenizer\n",
        "model_name = 'roberta-large'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "path = '/content/drive/MyDrive/SQUAD'\n",
        "\n",
        "with open(path+'/training_set.json') as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(path+'/error IDs.txt', 'r') as filename:\n",
        "  errors = filename.read().split('\\n')\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data, errors)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_eval_examples(raw_train_data, errors)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpEHU6MuNagH"
      },
      "source": [
        "def create_model():\n",
        "    ## BERT encoder\n",
        "    encoder = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "    ## QA Model\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(\n",
        "        input_ids = input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        "    ).last_hidden_state\n",
        "\n",
        "    start_logits = layers.Dense(1, use_bias=False, name='start_logit')(embedding)\n",
        "    start_logits = layers.Flatten(name = 'flatten_start')(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, use_bias=False, name = 'end_logit')(embedding)\n",
        "    end_logits = layers.Flatten(name = 'flatten_end')(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax, name = 'start_pred')(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax, name = 'end_pred')(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    sampled = tf.argmax(y_pred, axis=-1)\n",
        "    acc = 1 - tf.math.count_nonzero(tf.squeeze(tf.cast(y_true, tf.int64)) - sampled) / tf.cast(len(sampled), tf.int64)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53c5a7fc4e434e5d894cd1c71af84978",
            "796be84da21b4941a35f20e9567a1aa8",
            "e7f04cff294347249fcffa630135220d",
            "12290231470c4132bd65a82a1d40e7ba",
            "848eef3ec052492b84f05f97acfd7152",
            "87bdff13e0fa4aea9b7f16c678e3f47b",
            "01ccbd1f3e384eba960190675640a702",
            "d7ff1b85ea3848cbbf116c5d9735266d"
          ]
        },
        "id": "yNLaAxO48Fa3",
        "outputId": "09b50dbc-b685-4289-873b-90b760ebf287"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "saveDir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(saveDir):\n",
        "    os.makedirs(saveDir)\n",
        "chkpt = saveDir + '/' + model_name + '.hdf5'\n",
        "    # Create distribution strategy\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "\n",
        "ES_start = EarlyStopping(monitor='val_start_pred_accuracy', patience=2,verbose=1, mode='auto', restore_best_weights = True)\n",
        "ES_end = EarlyStopping(monitor='val_end_pred_accuracy', patience=2,verbose=1, mode='auto', restore_best_weights = True)\n",
        "\n",
        "cp_cb = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, \n",
        "                        save_best_only=False, mode='auto', \n",
        "                        save_weights_only=True)\n",
        "\n",
        "callbacks = [ES_end, ES_start, cp_cb]\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(optimizer=optimizer, loss=[loss, loss], metrics=[accuracy])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=200,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=1,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_eval,y_eval),\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53c5a7fc4e434e5d894cd1c71af84978",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=487203636.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilroberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at distilroberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_45 (InputLayer)           [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_46 (InputLayer)           [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model_3 (TFRobertaMo TFBaseModelOutputWit 82118400    input_45[0][0]                   \n",
            "                                                                 input_46[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 512, 1)       768         tf_roberta_model_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 512, 1)       768         tf_roberta_model_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "flatten_start (Flatten)         (None, 512)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_end (Flatten)           (None, 512)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "start_pred (Activation)         (None, 512)          0           flatten_start[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "end_pred (Activation)           (None, 512)          0           flatten_end[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 82,119,936\n",
            "Trainable params: 82,119,936\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "666/666 [==============================] - 256s 304ms/step - loss: 5.7538 - start_pred_loss: 2.9448 - end_pred_loss: 2.8091 - start_pred_accuracy: 0.3300 - end_pred_accuracy: 0.3489 - val_loss: 2.3946 - val_start_pred_loss: 1.1693 - val_end_pred_loss: 1.2253 - val_start_pred_accuracy: 0.6678 - val_end_pred_accuracy: 0.6801\n",
            "\n",
            "Epoch 00001: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 2/200\n",
            "666/666 [==============================] - 159s 239ms/step - loss: 2.5479 - start_pred_loss: 1.3178 - end_pred_loss: 1.2301 - start_pred_accuracy: 0.6175 - end_pred_accuracy: 0.6547 - val_loss: 2.1367 - val_start_pred_loss: 1.0579 - val_end_pred_loss: 1.0789 - val_start_pred_accuracy: 0.7036 - val_end_pred_accuracy: 0.7221\n",
            "\n",
            "Epoch 00002: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 3/200\n",
            "666/666 [==============================] - 159s 239ms/step - loss: 2.2268 - start_pred_loss: 1.1558 - end_pred_loss: 1.0710 - start_pred_accuracy: 0.6554 - end_pred_accuracy: 0.6935 - val_loss: 2.0435 - val_start_pred_loss: 1.0086 - val_end_pred_loss: 1.0349 - val_start_pred_accuracy: 0.7172 - val_end_pred_accuracy: 0.7258\n",
            "\n",
            "Epoch 00003: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 4/200\n",
            "666/666 [==============================] - 159s 238ms/step - loss: 1.9888 - start_pred_loss: 1.0410 - end_pred_loss: 0.9478 - start_pred_accuracy: 0.6850 - end_pred_accuracy: 0.7229 - val_loss: 1.9770 - val_start_pred_loss: 0.9989 - val_end_pred_loss: 0.9780 - val_start_pred_accuracy: 0.7205 - val_end_pred_accuracy: 0.7355\n",
            "\n",
            "Epoch 00004: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 5/200\n",
            "666/666 [==============================] - 159s 238ms/step - loss: 1.8385 - start_pred_loss: 0.9663 - end_pred_loss: 0.8723 - start_pred_accuracy: 0.7038 - end_pred_accuracy: 0.7448 - val_loss: 1.9450 - val_start_pred_loss: 0.9852 - val_end_pred_loss: 0.9599 - val_start_pred_accuracy: 0.7226 - val_end_pred_accuracy: 0.7467\n",
            "\n",
            "Epoch 00005: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 6/200\n",
            "666/666 [==============================] - 159s 239ms/step - loss: 1.7158 - start_pred_loss: 0.9064 - end_pred_loss: 0.8095 - start_pred_accuracy: 0.7162 - end_pred_accuracy: 0.7573 - val_loss: 1.9194 - val_start_pred_loss: 0.9752 - val_end_pred_loss: 0.9442 - val_start_pred_accuracy: 0.7355 - val_end_pred_accuracy: 0.7478\n",
            "\n",
            "Epoch 00006: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 7/200\n",
            "666/666 [==============================] - 159s 238ms/step - loss: 1.5972 - start_pred_loss: 0.8482 - end_pred_loss: 0.7490 - start_pred_accuracy: 0.7315 - end_pred_accuracy: 0.7727 - val_loss: 1.9186 - val_start_pred_loss: 0.9860 - val_end_pred_loss: 0.9326 - val_start_pred_accuracy: 0.7287 - val_end_pred_accuracy: 0.7588\n",
            "\n",
            "Epoch 00007: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 8/200\n",
            "666/666 [==============================] - 159s 238ms/step - loss: 1.4885 - start_pred_loss: 0.7940 - end_pred_loss: 0.6945 - start_pred_accuracy: 0.7447 - end_pred_accuracy: 0.7850 - val_loss: 1.9678 - val_start_pred_loss: 1.0137 - val_end_pred_loss: 0.9540 - val_start_pred_accuracy: 0.7286 - val_end_pred_accuracy: 0.7603\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00008: saving model to /content/saved_models/distilroberta-base.hdf5\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efaa21098d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2ruNXav4ovf",
        "outputId": "ff8f1963-85a7-4e6b-ac56-3f2bc7648d93"
      },
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -O test_set.json\n",
        "\n",
        "def create_test_examples(raw_data, errors):\n",
        "    squad_examples = []\n",
        "    skipped = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "              if qa['id'] not in errors:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers, qa['id']\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                if squad_eg.skip == False:\n",
        "                  squad_examples.append(squad_eg)\n",
        "                else: \n",
        "                  skipped.append(squad_eg.qid)\n",
        "    return squad_examples, skipped\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"attention_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "with open('test_set.json') as f:\n",
        "    raw_test_data = json.load(f)\n",
        "\n",
        "test_squad_examples, skipped = create_test_examples(raw_test_data, errors)\n",
        "x_test, y_test = create_inputs_targets(test_squad_examples)\n",
        "print(f\"{len(test_squad_examples)} evaluation points created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-25 15:00:59--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘test_set.json’\n",
            "\n",
            "test_set.json       100%[===================>]   4.63M  25.8MB/s    in 0.2s    \n",
            "\n",
            "2021-05-25 15:00:59 (25.8 MB/s) - ‘test_set.json’ saved [4854279/4854279]\n",
            "\n",
            "10462 evaluation points created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb5UUDLiF8Ij",
        "outputId": "39a22554-5136-4b1c-98cf-cc599085f267"
      },
      "source": [
        "predictions = model.predict(x_test, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "327/327 [==============================] - 16s 41ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUNXDmj9bK5w",
        "outputId": "b0c64d4f-a04e-418b-9059-21401d8f1747"
      },
      "source": [
        "num_samples = len(predictions[0])\n",
        "\n",
        "start, end = list(np.argmax(predictions, axis=-1).squeeze())\n",
        "lines_c = 0\n",
        "with open(\"dev_predictions.txt\",\"w\") as out:\n",
        "    out.write(\"{\")\n",
        "    for x in skipped:\n",
        "      out.write(f'''\"{x}\": \"42\",\\n''')\n",
        "    for ans_idx in range(num_samples):\n",
        "        if test_squad_examples[ans_idx].skip == False:\n",
        "          if end[ans_idx] == 0:\n",
        "              if ans_idx == num_samples-1:\n",
        "                  out.write(f'''\"{squad_test_examples[ans_idx].qid}\": \"\"''')\n",
        "              else:\n",
        "                  out.write(f'''\"{squad_test_examples[ans_idx].qid}\": \"\",\\n''')\n",
        "          else:\n",
        "              predicted_ans = tokenizer.decode(test_squad_examples[ans_idx].input_ids[start[ans_idx] : end[ans_idx]+1]).replace(\"\\n\",\" \")\n",
        "              if ans_idx == num_samples-1:\n",
        "                  out.write(f'''\"{test_squad_examples[ans_idx].qid}\": \"{predicted_ans.replace('\"',\"\")}\"''')\n",
        "              else:\n",
        "                  out.write(f'''\"{test_squad_examples[ans_idx].qid}\": \"{predicted_ans.replace('\"',\"\")}\",\\n''')\n",
        "        else:\n",
        "            out.write(f'''\"{test_squad_examples[ans_idx].qid}\": \"42\",\\n''')\n",
        "    out.write(\"}\")\n",
        "\n",
        "evaluation = !python3 evaluate.py test_set.json dev_predictions.txt\n",
        "print(evaluation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['{', '  \"exact\": 78.18353831598864,', '  \"f1\": 84.96487470436928,', '  \"total\": 10570,', '  \"HasAns_exact\": 78.18353831598864,', '  \"HasAns_f1\": 84.96487470436928,', '  \"HasAns_total\": 10570', '}']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRY1SamaJy9R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}